{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7cc52a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ImageDataAugmentor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend \u001b[38;5;28;01mas\u001b[39;00m K\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mA\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mImageDataAugmentor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_data_augmentor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataAugmentor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialise the EfficientNet Model for transfer learning\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ImageDataAugmentor'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import albumentations as A\n",
    "from ImageDataAugmentor.image_data_augmentor import ImageDataAugmentor\n",
    "\n",
    "import utils\n",
    "\n",
    "# Initialise the EfficientNet Model for transfer learning\n",
    "def EffNet(input_size, num_classess, pretrained_model, lr_rate, print_trainable_layers = False, print_model_summary = False):\n",
    "    # Get the EfficientNet Model\n",
    "    base_model = pretrained_model(\n",
    "        weights='imagenet',\n",
    "        input_shape = input_size,\n",
    "        include_top = False)\n",
    "    \n",
    "    # Keep the BatchNorm layer freeze, and unfreeze all other layers\n",
    "    def unfreeze_model(model, print_trainable, print_summary):\n",
    "        # unfreeze the layers while leaving BatchNorm layers frozen\n",
    "        for layer in model.layers[:]:\n",
    "            if isinstance(layer, layers.BatchNormalization):\n",
    "                layer.trainable = False\n",
    "\n",
    "        # Print trainable layer summary\n",
    "        if print_trainable:\n",
    "            for layer in model.layers:\n",
    "                print(layer, layer.trainable)\n",
    "        \n",
    "        # Print Model summary\n",
    "        if print_summary:\n",
    "            base_model.summary()\n",
    "\n",
    "    # Unfreeze the model\n",
    "    unfreeze_model(base_model, print_trainable_layers, print_model_summary)\n",
    "\n",
    "    # Add dense and output layer\n",
    "    model = keras.Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(layers.Flatten(name='top_flatten'))\n",
    "    model.add(layers.Dense(500, activation='relu', name='dense_500'))\n",
    "    model.add(layers.Dense(256, activation='relu', name='dense_256'))\n",
    "    model.add(layers.Dense(num_classess, activation='softmax', name='output_layer'))\n",
    "\n",
    "    # Initialise the optimizer and compile the model\n",
    "    optimizer = Adam(learning_rate = lr_rate)\n",
    "    model.compile(optimizer= optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # print the FC layer summary\n",
    "    if print_model_summary:\n",
    "        model.summary() \n",
    "\n",
    "    return model \n",
    "\n",
    "# Fit the model on training and validation dataset and star the training process.\n",
    "def train_model(model, train_generator, epoch, train_batch_size, validation_generator, validation_batch_size, train_step, valid_step,\n",
    "callback):\n",
    "    return model.fit(\n",
    "      train_generator,\n",
    "      epochs = epoch,\n",
    "      batch_size = train_batch_size,\n",
    "      validation_data = validation_generator,\n",
    "      validation_batch_size = validation_batch_size,\n",
    "      steps_per_epoch = train_step,\n",
    "      validation_steps = valid_step,\n",
    "      verbose = 1,\n",
    "      callbacks = callback)\n",
    "\n",
    "# Augment the dataset\n",
    "def augment_images(image_size):\n",
    "    transforms_train = A.Compose([\n",
    "        A.Transpose(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(p=0.5),\n",
    "        A.RandomBrightness(limit=0.2, p=0.5),\n",
    "        A.RandomContrast(limit=0.2, p=0.5),\n",
    "        A.OneOf([\n",
    "                A.MotionBlur(blur_limit=5),\n",
    "                A.MedianBlur(blur_limit=5),\n",
    "                A.GaussianBlur(blur_limit=5),\n",
    "                A.GaussNoise(var_limit=(5.0, 30.0))\n",
    "                ], p=0.7),\n",
    "        A.OneOf([\n",
    "                A.OpticalDistortion(distort_limit=1.0),\n",
    "                A.GridDistortion(num_steps=5, distort_limit=1.),\n",
    "                A.ElasticTransform(alpha=3)\n",
    "                ], p=0.7),\n",
    "        A.CLAHE(clip_limit=4.0, p=0.7),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
    "        A.Resize(width=image_size, height=image_size),\n",
    "        A.Cutout(max_h_size= int(image_size*0.375), max_w_size= int(image_size*0.375), num_holes=1, p=0.7),\n",
    "        A.Normalize(),\n",
    "    ])\n",
    "\n",
    "    transforms_val = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "    transforms_test = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "    return transforms_train, transforms_val, transforms_test\n",
    "\n",
    "# Generate the training and validation augmented dataset. \n",
    "def data_generator(seed, transforms_train, transforms_val, label, \n",
    "train_path, image_resize, train_batch_size, validation_batch_size):\n",
    "\n",
    "    train_datagen = ImageDataAugmentor(\n",
    "        augment = transforms_train,\n",
    "        preprocess_input = None, \n",
    "        seed = seed,\n",
    "        validation_split = 0.2) # Define validation split i.e 20% data is used for validation\n",
    "\n",
    "    valid_datagen = ImageDataAugmentor(\n",
    "            augment = transforms_val,\n",
    "            preprocess_input = None, \n",
    "            seed = seed,\n",
    "            validation_split = 0.2) # Define validation split i.e 20% data is used for validation\n",
    "\n",
    "    # Flow training images using train_datagen generator\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe = label,  \n",
    "            directory = train_path,\n",
    "            x_col = 'image',\n",
    "            y_col = 'diagnosis',\n",
    "            target_size= image_resize, \n",
    "            batch_size = train_batch_size,\n",
    "            subset = 'training',\n",
    "            class_mode='categorical',\n",
    "            validate_filenames = False)\n",
    "\n",
    "    validation_generator = valid_datagen.flow_from_dataframe(\n",
    "            dataframe = label, \n",
    "            directory = train_path,\n",
    "            x_col = 'image',\n",
    "            y_col = 'diagnosis',\n",
    "            target_size= image_resize,\n",
    "            batch_size = validation_batch_size,\n",
    "            subset = 'validation',\n",
    "            class_mode='categorical',\n",
    "            validate_filenames = False)\n",
    "\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267e4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
